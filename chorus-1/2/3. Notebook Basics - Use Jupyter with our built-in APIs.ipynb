{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 2px; padding-left: 0px\">\n",
    "<img src=\"http://alpinedata.com/wp-content/themes/alpine/library/images/logo.png\" width=\"250px\" style=\"display: inline-block; margin-top: 2px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "# Jupyter Notebook in Chorus - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Documentation on Alpine-Jupyter integration:<br>\n",
    "\n",
    "<a href=\"https://alpine.atlassian.net/wiki/display/V6/Using+Alpine+Chorus+Notebooks\">https://alpine.atlassian.net/wiki/display/V6/Using+Alpine+Chorus+Notebooks</a><br>\n",
    "\n",
    "This notebook shows you how to interact with a Jupyter notebook in Chorus using the ChorusCommander API that is provided by Alpine to easily interact with your data.\n",
    "\n",
    "This library is automatically imported when the notebook starts.\n",
    "It is imported into the notebook as an object called `cc`.<br><br>\n",
    "\n",
    "<font><b>Instructions:</b><br> <br>\n",
    "1) To run Jupyter notebooks within Chorus, you need to set up a dedicated server and make all the needed configurations.<br>\n",
    "\n",
    "2) This script illustrates how to use two data sources (Database and HDFS) - which the user will have to modify in order to run<br>\n",
    "\n",
    "3) <i>(Once 1-2 is completed)</i> DO NOT modify/run this script in the current workspace. You should copy it to your own workspace (using the Copy button after closing the notebook).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Methods and Help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods currently implemented by the ChorusCommander API:\n",
    "\n",
    "(You can type `cc.` and press Tab to see all these methods by auto-completion)\n",
    "- `datasource_name`\n",
    "- `database_name`\n",
    "- `hdfs_ls`\n",
    "- `read_file_csv`\n",
    "- `write_file_csv`\n",
    "- `write_file`\n",
    "- `sql_execute` (note: use this one if writing to an Oracle database, our built-in methods won’t work)\n",
    "- `read_table`\n",
    "- `create_table`\n",
    "- `write_table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call any of the following methods, use<b> `cc.method_name(parameters)`</b>. \n",
    "You can display documentation about them from within the notebook by typing <b>`help(method_name)`</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method read_file_csv in module commander_lib.ChorusCommander:\n",
      "\n",
      "read_file_csv(self, file_path, sep=',', delimiter=None, header=None, names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression=None, thousands=None, decimal='.', lineterminator=None, quotechar='\"', quoting=0, escapechar=r'\\', comment=None, encoding='utf-8', dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None, **kwargs) method of commander_lib.ChorusCommander.ChorusCommander instance\n",
      "    Accesses a file on HDFS, and reads it into a DataFrame in pandas\n",
      "    using the ``pandas.read_csv`` function. This function also supports\n",
      "    optionally iterating or breaking the file into chunks.\n",
      "    \n",
      "    Additional information can be found in the\n",
      "    `IO Tools help <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\n",
      "    \n",
      "    Args:\n",
      "        file_path: The full path of the file in HDFS.\n",
      "        sep (str): Delimiter to use. Default is ','. If ``sep`` is\n",
      "            ``None``, the function will try to automatically determine\n",
      "            this. Separators longer than 1 character and different from\n",
      "            's+' will be interpreted as regular expressions. This will\n",
      "            force use of the Python parsing engine and will ignore quotes\n",
      "            in the data. Regex example: 'rt'\n",
      "        delimiter (str): Alternative argument name for ``sep``. Default is\n",
      "            ``None``.\n",
      "        header (int or list of ints): Row number(s) to use as the column\n",
      "            names. This number also refers to the start of the data. The\n",
      "            default value is 0 if no numbers are passed. To replace\n",
      "            existing names, use ``header=0``. The header can be a list of\n",
      "            integers that specify row locations for a mutli-index on the\n",
      "            columns. For example, the list [0,1,3] would skip row 2. Note\n",
      "            that this parameter ignores commented lines and empty lines if\n",
      "            ``skip_blank_lines=True``. In this case, ``header=0`` denotes\n",
      "            the first line of data rather than the first line of the file.\n",
      "            Default value is 'infer'; this means that the header row will\n",
      "            be automatically inferred.\n",
      "        names: List of column names to use. Default value is ``None``. If\n",
      "            the file contains no header row, then explicitly pass\n",
      "            ``header=None``.\n",
      "        index_col (int, sequence, or False): Column to use as the row\n",
      "            labels of the DataFrame. If a sequence is given, a MultiIndex\n",
      "            is used. If you have a malformed file with delimiters at the\n",
      "            end of each line, use ``index_col=False`` to force pandas _not_\n",
      "            to use the first column as the index for row names. Default\n",
      "            value is ``None``.\n",
      "        usecols (array-like): Return a subset of the columns. All elements\n",
      "            in this array must either be positional (such as integer\n",
      "            indices into the document columns) or strings that correspond\n",
      "            to column names provided either by the user in ``names`` or\n",
      "            inferred from the document header row(s). For example, a valid\n",
      "            ``usecols`` parameter would be [0,1,2] or ['foo','bar','baz'].\n",
      "            Using this parameter results in much faster parsing time and\n",
      "            lower memory usage. Default value is ``None``.\n",
      "        squeeze (boolean): If the parsed data only contains one column,\n",
      "            then return a Series. Default value is ``False``.\n",
      "        prefix (str): Prefix to add column numbers when there is no header.\n",
      "            For example, 'X' would be added to each column number to\n",
      "            become 'X0','X1',...\n",
      "            Default value is ``None``.\n",
      "        mangle_dupe_cols (boolean): Duplicate columns will be specified as\n",
      "            'X.0'...'X.N', rather than 'X'...'X'\n",
      "            Default value is ``True``.\n",
      "        dtype (Type name or dict of column -> type): Data type for data or\n",
      "            columns. E.g. ``{‘a’: np.float64, ‘b’: np.int32}`` (Unsupported\n",
      "            with engine=’python’). Use a ``str`` or ``object`` to preserve\n",
      "            and not interpret dtype.\n",
      "        engine (Optional): ``{'c', 'python'}``. Parser enginge to use. The\n",
      "            C engine is faster, while the python engine is currently more\n",
      "            feature-complete.\n",
      "        converters (dict): Dict of functions for converting values in\n",
      "            in certain columns. Keys can either be integers or column\n",
      "            labels. Default value is ``None``.\n",
      "        true_values (list): Values to consider as ``True``. Default value\n",
      "            is ``None``.\n",
      "        false_values (list): Values to consider as ``False``. Default value\n",
      "            is ``None``.\n",
      "        skipinitialspace (boolean): Skip spaces after delimiter. Default\n",
      "            value is ``False``.\n",
      "        skiprows (list-like or integer): Line numbers to skip (0-indexed)\n",
      "            or number of lines to skip (integer) at the start of the file.\n",
      "            Default value is ``None``.\n",
      "        skipfooter (int): Number of lines at the bottom of hte file to skip\n",
      "            (Unsupported with ``engine=c``). Default value is 0.\n",
      "        nrows (int): Number of rows of the file to read. Useful for reading\n",
      "            pieces of large files. Default value is ``None``.\n",
      "        na_values (str or list-like or dict): Additional strings to\n",
      "            recognize as NA/NaN. If dict passed, specific per-column NA\n",
      "            values. By default the following values are interpreted as NaN:\n",
      "            '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN',\n",
      "            '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'nan'.\n",
      "            Default value is ``None``.\n",
      "        keep_default_na (bool): If ``na_values`` are specified and\n",
      "            ``keep_default_na`` is ``False``, the default NaN values are\n",
      "            overriden, otherwise they are appended to. Default value is\n",
      "            ``True``.\n",
      "        na_filter (boolean): Detect missing value markers (empty strings\n",
      "            and the value of ``na_values``). In data without any NAs,\n",
      "            passing ``na_filter=False`` can impprove the performance of\n",
      "            reading a large file. Default value is ``True``.\n",
      "        verbose (boolean): Indicate the number of NA values placed in\n",
      "            non-numeric columns. Default value is ``False``.\n",
      "        skip_blank_lines (boolean): If ``True``, skip over blank lines\n",
      "            rather than interpreting them as NaN values. Default value is\n",
      "            ``True``.\n",
      "        parse_dates (boolean or list of ints or list of lists or dict):\n",
      "            - If boolean: If ``True``, then try parsing the index.\n",
      "            - If list of ints or names: For example, if [1,2,3], then try\n",
      "            parsing columns 1,2,3 each as a separate date column.\n",
      "            - If list of lists: For example, if [[1,3]], then combine\n",
      "            columns 1 and 3 and parse as a single date column.\n",
      "            - If dict: For example, if {‘foo’ : [1, 3]}, then parse columns\n",
      "            1,3 as date and call result ``foo``. Note: a fast-path exists\n",
      "            for iso8601-formatted dates.\n",
      "            - Default value is ``False``.\n",
      "        infer_datetime_format (boolean): If ``True`` and ``parse_dates`` is\n",
      "            enabled for a column, then attempt to infer the datetime format\n",
      "            to speed up the processing. Default value is ``False``.\n",
      "        keep_date_col (boolean): If ``True`` and ``parse_dates`` specifies\n",
      "            combining multiple columns, then keep the original columns.\n",
      "            Default value is ``False``.\n",
      "        date_parser (function): Function to use for converting a sequence\n",
      "            of string columns to an array of datetime instances. The\n",
      "            default uses ``dateutil.parser.parser`` to do the conversion.\n",
      "            Pandas will try to call ``date_parser`` in three different\n",
      "            ways, advancing to the next if an exception occurs:\n",
      "            1) Pass one or more arrays (as defined by ``parse_dates``) as\n",
      "            arguments;\n",
      "            2) concatenate (row-wise) the string values from the columns\n",
      "            defined by ``parse_dates`` into a single array and pass that;\n",
      "            and\n",
      "            3) call ``date_parser`` once for each row using one or more\n",
      "            strings (corresponding to the columns defined by\n",
      "            ``parse_dates``) as arguments.\n",
      "            Default value is ``None``.\n",
      "        dayfirst (boolean): DD/MM format dates. This is the international\n",
      "            and European format. Default value is ``False``.\n",
      "        iterator (boolean): Return a ``TextFileReader`` object for\n",
      "            iteration or getting file chunks with ``get_chunk()``. Default\n",
      "            value is ``False``.\n",
      "        chunksize (int): Return a ``TextFileReader`` object for iteration.\n",
      "            See IO Tools docs for more information on iterator and\n",
      "            chunksize. Default value is ``None``.\n",
      "        compression: ``{‘infer’, ‘gzip’, ‘bz2’, None}``. For on-the-fly\n",
      "            decompression of on-disk date. If 'infer', then use gzip or bz2\n",
      "            if ``filepath_or_buffer`` is a string ending in '.gz' or\n",
      "            '.bz2', respectively. No decompression otherwise. Set to\n",
      "            ``None`` for no decompression. Default value is 'infer'.\n",
      "        thousands (str): Thousands separator. Default value is ``None``.\n",
      "        decimal (str): Character to recognize as a decimal point. (For\n",
      "            example, use ',' for European data). Default value is '.'\n",
      "        lineterminator (str - length 1): Character to break file into\n",
      "            lines. Only valid with C parser. Default value is ``None``.\n",
      "        quotechar (str - length 1, Optional): The character used to denote\n",
      "            the start and end of a quoted item. Quoted items can include\n",
      "            the delimiter and it will be ignored.\n",
      "        quoting (int or csv.QUOTE_* instance): Control field quoting\n",
      "            behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "            QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or\n",
      "            QUOTE_NONE (3). Default (``None``) results in QUOTE_MINIMAL\n",
      "            behavior.\n",
      "        escapechar (str - length 1): One-character string used to\n",
      "            escapechar delimiter when quoting is QUOTE_NONE. Default value\n",
      "            is ``\\``.\n",
      "        comment (str): A character which indicates the remainder of line\n",
      "            should not be parsed. If this is found at the beginning of a\n",
      "            line, the line will be ignored altogether. This parameter must\n",
      "            be a single character. Like empty lines (as long as\n",
      "            ``skip_blank_lines=True``), fully commented lines are ignored\n",
      "            by the parameter header but not by ``skiprows``. For example,\n",
      "            if ``comment='#``, parsing\n",
      "            '#empty\n",
      "            a,b,c\n",
      "            1,2,3'\n",
      "            with ``header=0`` will result in 'a,b,c' being treated as the\n",
      "            header. Default value is ``None``.\n",
      "        encoding (str): Encoding to use for UTF when reading/writing\n",
      "            (ex. ‘utf-8’). List of Python standard encodings can be found\n",
      "            at\n",
      "            https://docs.python.org/3/library/codecs.html#standard-encodings\n",
      "            Default value is ``utf-8``.\n",
      "        dialect (str or csv.Dialect): If ``None``, then it defaults to the\n",
      "            Excel dialect. Ignored if ``sep`` is longer than 1 char. See\n",
      "            ``csv.Dialect`` documentation for more details. Default value\n",
      "            is ``None``.\n",
      "        tupleize_cols (boolean): Leave a list of tuples on columns as-is.\n",
      "            The default is to convert to a MultiIndex on the columns.\n",
      "            Default value is ``False``.\n",
      "        error_bad_lines (boolean): Lines with too many fields (e.g. a csv\n",
      "            line with too many commas) will by default cause an exception\n",
      "            to be raised, and no DataFrame will be returned. If ``False``,\n",
      "            then these \"bad lines\" will be dropped from the DataFrame that\n",
      "            is returned. (Only valid with C parser). Default value is\n",
      "            ``True``.\n",
      "        warn_bad_lines (boolean): If ``error_bad_lines`` is ``False``, and\n",
      "            ``warn_bad_lines`` is ``True``, a warning for each \"bad line\"\n",
      "            will be output. (Only valid with C parser). Default value is\n",
      "            ``True``.\n",
      "        **kwargs: arbitrary key-value pairs. For example: ``verify=False``\n",
      "            means do not verify the SSL cert when SSL is enabled.\n",
      "    \n",
      "    Returns:\n",
      "        pandas.DataFrame: return a pandas DataFrame.\n",
      "    \n",
      "    Raises:\n",
      "        Exception: An error occured when contacting the Chorus API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "help(cc.read_file_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to import standard libraries such as pandas type <b>`import pandas as pd`</b>  (or whatever alias you choose). You can find a list of pre-installed libraries [here](https://alpine.atlassian.net/wiki/display/V6/Using+Alpine+Chorus+Notebooks?preview=/105840664/110067972/pypackagelist.txt).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the library is not pre-installed, you can install it using <b>`!pip install library_name`</b> \n",
    "from the notebook.\n",
    "\n",
    "However, if the Docker container for the notebook is destroyed, you may need to install the package again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): seaborn in /home/chorus/ChorusCommander/.pyenv/versions/2.7.6/envs/project_env/lib/python2.7/site-packages\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chorus/ChorusCommander/.pyenv/versions/2.7.6/envs/project_env/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "#Example: importing seaborn\n",
    "\n",
    "!pip install seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data associated with your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily import data associated with your workspace. You can include both Hadoop files and database tables within the same notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the <b>Data button</b> at the top --> <b>\"Import Dataset into Notebook\"</b>  and select the dataset to import.<br>\n",
    "The notebook will automatically detect the data source and use the right method to import your data. ChorusCommander reads it into a DataFrame in pandas using the ``pandas.read_csv`` function. It then saves the DataFrame to a variable called ``df_filename``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chorus/ChorusCommander/.pyenv/versions/2.7.6/envs/project_env/lib/python2.7/commander_lib/ChorusCommander.py:559: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  return df.convert_objects(convert_numeric=True)\n"
     ]
    }
   ],
   "source": [
    "#Importing the database table \"credit\" associated with my workspace.\n",
    "#The following code was AUTO-GENERATED when doing what is described above\n",
    "\n",
    "cc.datasource_name = 'Postgres_Demo'\n",
    "df_credit = cc.read_table(table_name='credit_100rows', schema_name='public', database_name='demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>times90dayslate</th>\n",
       "      <th>revolving_util</th>\n",
       "      <th>debt_ratio</th>\n",
       "      <th>credit_lines</th>\n",
       "      <th>monthly_income</th>\n",
       "      <th>times30dayslate_2years</th>\n",
       "      <th>srsdlqncy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.675</td>\n",
       "      <td>6</td>\n",
       "      <td>2820.658562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1055</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.482</td>\n",
       "      <td>5</td>\n",
       "      <td>4069.634891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1537</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.433</td>\n",
       "      <td>7</td>\n",
       "      <td>1478.238497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1593</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.133</td>\n",
       "      <td>7</td>\n",
       "      <td>1517.769146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1846</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.287</td>\n",
       "      <td>4</td>\n",
       "      <td>2005.337706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  times90dayslate  revolving_util  debt_ratio  credit_lines  \\\n",
       "0    91                0            0.35       0.675             6   \n",
       "1  1055                0            0.18       0.482             5   \n",
       "2  1537                0            0.29       0.433             7   \n",
       "3  1593                0            0.32       0.133             7   \n",
       "4  1846                0            0.23       0.287             4   \n",
       "\n",
       "   monthly_income  times30dayslate_2years  srsdlqncy  \n",
       "0     2820.658562                       0          0  \n",
       "1     4069.634891                       0          0  \n",
       "2     1478.238497                       0          0  \n",
       "3     1517.769146                       0          0  \n",
       "4     2005.337706                       0          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizing the first rows of the datast imported\n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import several datasets from multiple data sources in the same notebook. \n",
    "\n",
    "After importing data from our Postgres database, we now import a file stored in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INQRY_LOG_ID</th>\n",
       "      <th>INQRY_WEBST_ID</th>\n",
       "      <th>INQRY_TRMT_CTGY_CD</th>\n",
       "      <th>INQRY_APFX_CD</th>\n",
       "      <th>INQRY_RADIUS_NUM</th>\n",
       "      <th>INQRY_ZIP_CD</th>\n",
       "      <th>DURATION</th>\n",
       "      <th>VALID_FLAG</th>\n",
       "      <th>OPRTN_CD</th>\n",
       "      <th>CITY_NM</th>\n",
       "      <th>ST_ALPHA_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>19501</td>\n",
       "      <td>WPV</td>\n",
       "      <td>10</td>\n",
       "      <td>80122.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>PLAYA DEL RAE</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>19501</td>\n",
       "      <td>WPV</td>\n",
       "      <td>15</td>\n",
       "      <td>80516.0</td>\n",
       "      <td>5.612</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>CINCINNATI</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>23456</td>\n",
       "      <td>WPV</td>\n",
       "      <td>20</td>\n",
       "      <td>80108.0</td>\n",
       "      <td>33.498</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>34565</td>\n",
       "      <td>BJN</td>\n",
       "      <td>25</td>\n",
       "      <td>80122.0</td>\n",
       "      <td>123.453</td>\n",
       "      <td>Y</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>43241</td>\n",
       "      <td>BJN</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.341</td>\n",
       "      <td>Y</td>\n",
       "      <td>R</td>\n",
       "      <td>DENVER</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INQRY_LOG_ID  INQRY_WEBST_ID  INQRY_TRMT_CTGY_CD INQRY_APFX_CD  \\\n",
       "0             1              55               19501           WPV   \n",
       "1             2              55               19501           WPV   \n",
       "2             3              55               23456           WPV   \n",
       "3             4              66               34565           BJN   \n",
       "4             5              66               43241           BJN   \n",
       "\n",
       "   INQRY_RADIUS_NUM  INQRY_ZIP_CD  DURATION VALID_FLAG OPRTN_CD  \\\n",
       "0                10       80122.0     0.050          Y        P   \n",
       "1                15       80516.0     5.612          Y        P   \n",
       "2                20       80108.0    33.498          Y        P   \n",
       "3                25       80122.0   123.453          Y        R   \n",
       "4                50           NaN     3.341          Y        R   \n",
       "\n",
       "         CITY_NM ST_ALPHA_CD  \n",
       "0  PLAYA DEL RAE          CA  \n",
       "1     CINCINNATI          OH  \n",
       "2            NaN          CA  \n",
       "3            NaN          NE  \n",
       "4         DENVER          CO  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing data 'search_data_csv' stored in Hadoop as a pandas DataFame: (code auto-generated)\n",
    "\n",
    "hdfs_path = '/search_data.csv'\n",
    "cc.datasource_name = 'CDH54_HA'\n",
    "df_search_data_csv = cc.read_file_csv(hdfs_path, header=0)\n",
    "\n",
    "df_search_data_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data to a database or HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some simple transformations on one of our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Casting columns to numeric and selecting customers with more than 3 credit_lines:\n",
    "\n",
    "df_credit[['times90dayslate', 'revolving_util','debt_ratio','credit_lines']] = df_credit[['times90dayslate',\n",
    "                                            'revolving_util','debt_ratio','credit_lines']].apply(pd.to_numeric)\n",
    "\n",
    "df_credit_filtered = df_credit[df_credit['credit_lines'] > 3][['id', 'credit_lines']]\n",
    "\n",
    "#print \"Number of customers with more than 3 credit lines: {0}\".format(df_credit_filtered.id.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id              int64\n",
      "credit_lines    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Checking the data types\n",
    "print(df_credit_filtered.dtypes)\n",
    "col_info =  [{\"id\":\"varchar(50)\"}, {\"credit_lines\":\"int\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interacting with multiple data sources within the same notebook, you need to reinitialize the\n",
    "``cc.datasource_name`` variable before interacting with the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc.datasource_name = 'Postgres_Demo'\n",
    "\n",
    "#Creating table is done automatically when calling write_table\n",
    "cc.write_table(data_frame=df_credit_filtered,\n",
    "               table_name='credit_filtered',\n",
    "               column_info=[{\"id\":\"varchar(50)\"}, {\"credit_lines\":\"int\"}],\n",
    "               schema_name='public',\n",
    "               database_name='demo',\n",
    "               drop_if_exists=True,\n",
    "               limit=100000)\n",
    "\n",
    "# New table is stored in the database and can be used in an Alpine workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the HDFS path to store the file\n",
    "hdfs_path_new = '/datasets/credit_new'\n",
    "\n",
    "# Reinitialize the data source name before interacting with the data source \n",
    "cc.datasource_name = 'CDH54_HA'\n",
    "\n",
    "# Use cc.write_file_csv if you have a DataFrame to store as a CSV file\n",
    "cc.write_file_csv(data_frame=df_credit_filtered,\n",
    "              file_path=hdfs_path_new,\n",
    "              overwrite_exists=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not working with a DataFrame or you don't want to store results in CSV format, you can use our \n",
    "more general built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Also could use (more general method to use for any type of content):\n",
    "cc.write_file(content=df_credit_filtered.to_csv(),\n",
    "              file_path=hdfs_path_new,\n",
    "              overwrite_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has now been stored to HDFS and can be used in an Alpine workflow!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>And of course... you can use all existing functionalities available in a Jupyter notebook (with Python kernel) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
