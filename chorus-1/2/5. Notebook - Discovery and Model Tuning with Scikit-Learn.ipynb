{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 2px; padding-left: 0px\">\n",
    "<img src=\"http://alpinedata.com/wp-content/themes/alpine/library/images/logo.png\" width=\"250px\" style=\"display: inline-block; margin-top: 2px;\">\n",
    "</div>\n",
    "\n",
    "# Institutional Holdings Analysis - Model Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used for quick discovery and model tuning (finding best tuning parameters with cross-validation).\n",
    "\n",
    "### Libraries\n",
    "- Seaborn/matplotlib for visualizations and heatmaps.\n",
    "- Scikit-learn for cross-validation and grid search tuning of Elastic Net Logistic Regression.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1) To run Jupyter notebooks within Chorus, you need to set up a dedicated server and make all the needed configurations. See [our installation instructions](https://alpine.atlassian.net/wiki/display/V6/How+to+Install+Jupyter+Notebook).<br>\n",
    "\n",
    "2) <i>(Once 1 is completed)</i> DO NOT modify/run this script in the current workspace. You should copy it to your own workspace (using the Copy button after closing the notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import cross_validation, metrics, linear_model\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "#from mpltools import style\n",
    "if sys.version_info.major < 3:\n",
    "    import compiler\n",
    "%pylab inline\n",
    "plt.style.use('ggplot')\n",
    "#input_buy_in_or_sell_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and discovery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.datasource_name = 'Postgres_Demo'\n",
    "df_input_buyin_or_sellouts = cc.read_table(table_name='input_buyin_or_sellouts', \n",
    "                                           schema_name='public', database_name='demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_input_buyin_or_sellouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary = data.describe() # 10k rows\n",
    "summary.transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.buyin_or_sellou.value_counts().plot(kind = 'bar', color = '#728FCE', title = \n",
    "                                          \"Distribution of the classes in the training set\")\n",
    "# Classes are pretty balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering variables and null value checks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering irrelevant columns for classification based on our knowledge of the data\n",
    "\n",
    "input_filtered = data.drop(['ticker_QuandlInstitutionalShareholderMetrics',\n",
    "                            'comp_name_QuandlInstitutionalShareholderMetrics',\n",
    "                            'exchange_QuandlInstitutionalShareholderMetrics',\n",
    "                            'pct_total_portfolio_q0_q1_change','q0_q1_change','q0_q1_pct_change',\n",
    "                            'inst_nbr','inst_name','mgr_nbr','inst_loc','inst_addr',\n",
    "                            'inst_city_state','inst_post_code','inst_phone_nbr','last_change_date',\n",
    "                            'fund_ticker','inst_url','fund_mgmt_city','fund_mgmt_state','fund_mgmt_name',\n",
    "                            'shares_held_q0','fqe_date_q0','fqe_date_q1','fqe_date_q2','fqe_date_q3',\n",
    "                            'last_close_date','last_sec_filing_date','last_sec_filing_type',\n",
    "                            'shares_held_pct','shares_change',\n",
    "                            'shares_change_pct','inst_cik'], inplace = False, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total null values: {0}\".format(input_filtered.isnull().values.sum()))\n",
    "print(\"Columns containing null values are: \")\n",
    "\n",
    "null_values = pd.concat([input_filtered.dtypes,input_filtered.isnull().any()],\n",
    "                        axis=1, join_axes=[input_filtered.dtypes.index])\n",
    "NA_DF = null_values[null_values[1] == True]\n",
    "NA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in last_sec_filing_shares {0}\".format(input_filtered[['last_sec_filing_shares']]\n",
    "                                                         .isnull().values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filtered = input_filtered[pd.notnull(input_filtered['last_sec_filing_shares'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remaining null values? {0}\".format(input_filtered.isnull().any().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting categorical column to binary features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check catgorical column in the features selected\n",
    "input_filtered.dtypes.value_counts().plot(kind = 'barh', color = '#728FCE', title = 'Distribution of feature types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object columns need to be converted to binary features to train the Elastic Net Logistic Regression with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check categorical columns in our features\n",
    "\n",
    "types_df = pd.DataFrame({'dtypes': input_filtered.dtypes, 'names': input_filtered.columns})\n",
    "types_df[types_df['dtypes'] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at categorical columns to make sure they do not represent IDs\n",
    "\n",
    "test = input_filtered[['size_','code_13f_flag','fund_flag','mgr_flag',\n",
    "                       'etf_flag','invst_style']]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical var to dummy variables\n",
    "\n",
    "final_input = pd.get_dummies(input_filtered)\n",
    "final_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just taking the first 21 variables as an example to plot correlations\n",
    "cor = final_input[:21].corr()\n",
    "\n",
    "s = cor.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\")\n",
    "so.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the correlation heatmap\n",
    "\n",
    "f, ax = plt.subplots(figsize=(13, 11))\n",
    "ax.set_title(\"Correlations between First 20 features and dependent variable\")\n",
    "sns.heatmap(cor, vmax=.3,\n",
    "            square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some independent features seem to be strongly correlated, which might impact the performance of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create arrays of X_features and Y_predict to train the model\n",
    "\n",
    "columns_all = list(final_input.columns)\n",
    "columns = [col for col in columns_all if col not in ('buyin_or_sellou')]\n",
    "result = 'buyin_or_sellou'\n",
    "# convert to np array\n",
    "X_features = final_input[columns].values \n",
    "Y_status = final_input[result].values\n",
    "\n",
    "print(\"X_shape: %s, Y_shape: %s\" % X_features.shape, Y_status.shape) # check #rows, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search on alphas and l1_ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Elastic Net algorithm with different tuning parameters in a grid search\n",
    "# that returns the best tuning parameters\n",
    "# Grid search configured here with 3-fold cross validation for each pair of parameters\n",
    "\n",
    "lambdas = list(np.logspace(-5,2,5)) # we test 5 values from 10E-5 to 10E2\n",
    "l1_ratios = [0, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "\n",
    "tuned_parameters = {'alpha': lambdas, 'l1_ratio': l1_ratios}\n",
    "\n",
    "grid = GridSearchCV(linear_model.ElasticNet(normalize = False,\n",
    "                                            max_iter = 100000), \n",
    "                    tuned_parameters, scoring='roc_auc', cv=3, n_jobs=1, verbose=2)\n",
    "grid.fit(X_features, Y_status)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting AUC manually for <> tuning parameters and drawing a 3D chart to better visualize results\n",
    "\n",
    "# taking AUC as evaluation metric - we could also pick accuracy \n",
    "\n",
    "lambdas1 = list(np.logspace(-5,2,5)) # 10 values from 10E-5 to 10E2\n",
    "l1ratios1 = [1e-5, 1e-2, 0.1, 0.4, 0.7, 1]\n",
    "alphas = lambdas1\n",
    "l1_ratios = l1ratios1\n",
    "AUC = [[] for i in range(0,len(alphas))]\n",
    "\n",
    "for a in alphas:\n",
    "    for l1_rat in l1_ratios:\n",
    "        # Create elastic net instance:\n",
    "        lor = linear_model.ElasticNet(alpha=a, l1_ratio = l1_rat, max_iter = 10000)\n",
    "        # this returns the k AUC for the k-fold cross-validation (3 folds here)\n",
    "        CVauc = cross_validation.cross_val_score(lor, X_features, Y_status, \n",
    "                                                 scoring='roc_auc', n_jobs = 1, verbose =2 )\n",
    "        # taking the average of auc (for 3-fold cv)\n",
    "        CV_AUC = np.mean(CVauc)\n",
    "        AUC[alphas.index(a)].append(CV_AUC)\n",
    "\n",
    "max_AUCs =[] # list of max AUCs (associated with best alpha for each d value)\n",
    "\n",
    "for i in range(0,len(alphas)):\n",
    "    max_AUCs.append(max(AUC[i]))\n",
    "    \n",
    "    \n",
    "for a in alphas:\n",
    "    print(\"AUCs for alpha = {alpha}: {AUC_vals}\".format(alpha = a, AUC_vals = AUC[alphas.index(a)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[max_AUCs.index(max(max_AUCs))]\n",
    "best_l1_ratio = l1_ratios[AUC[alphas.index(best_alpha)].index(max(AUC[alphas.index(best_alpha)]))]\n",
    "best_AUC = max_AUCs[alphas.index(best_alpha)]\n",
    "\n",
    "print('Best alpha: {0}'.format(best_alpha))\n",
    "print('Best l1_ratio: {0}'.format(best_l1_ratio))\n",
    "print(\"\\nBest AUC: {0}\".format(best_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Tuning parameters alpha and l1_ration vs AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.rainbow(np.linspace(0,1, len(alphas)))\n",
    "\n",
    "fig = plt.figure(figsize(15,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for alpha, c in zip(alphas, colors):\n",
    "    ax.scatter(l1_ratios,[alpha for i in xrange(len(l1_ratios))],AUC[alphas.index(alpha)],\n",
    "               color = c, marker = \"o\", s=60)\n",
    "\n",
    "ax.set_xlabel('l1_ratio')\n",
    "ax.set_ylabel('alpha')\n",
    "ax.set_zlabel('AUC')\n",
    "plt.title(\"AUC vs shrinkage parameters (l1_ratio,alpha)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Results:</b> We should set alpha = 1E2 and l1ratio = 1 in our Alpine workflow to train Elastic Net Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
