{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from functools import reduce\n",
    "import os\n",
    "os.environ['HADOOP_USER_NAME'] = 'hdfs'\n",
    "\n",
    "# Modify the line below to change the number of executors and the amount of memory that they use \n",
    "# You can run Spark in local mode by changing the '--master' value to 'local', and setting the 'YARN_CONF_DIR' to ''\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local pyspark-shell\"\n",
    "\n",
    "# os.environ['YARN_CONF_DIR'] = '/home/chorus/ChorusCommander/hdfs_configs/2-10.0.0.244'\n",
    "os.environ['YARN_CONF_DIR'] = ''\n",
    "# Each worker node in the cluster needs Python 2.7.\n",
    "# If this is not the default Python on the node, provide the Python path here\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/data/opt/cloudera/parcels/Anaconda-4.0.0/bin/python'\n",
    "\n",
    "# This will stop the SparkContext if there is one left over from a different notebook execution\n",
    "try:\n",
    "    sc.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "APP_NAME = 'PysparkDemo.ipynb-sxie'\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file from HDFS [avalanche_cdh5_ha] \n",
    "rawRDD = sc.textFile(\"/data/demo_data/credit_orig.csv\").zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the CSV: extract header, add a row index column, [assuming all numeric values for data]\n",
    "parseStringIntoList = lambda s, t: [t-1] + [float(x) for x in s.split(\",\")]\n",
    "header = [\"RowIdx\"] + rawRDD.take(1)[0][0].split(\",\")\n",
    "data = rawRDD.filter(lambda p: p[1]>0).map(lambda p: parseStringIntoList(p[0], p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few lines of data RDD\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to create Spark DataFrame and run SparkSQL queries\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame with columns named as per header extracted above\n",
    "raw_df = sqlContext.createDataFrame(data)\n",
    "old_columns = raw_df.schema.names\n",
    "formatted_df = reduce(lambda sparkdf, idx: sparkdf.withColumnRenamed(old_columns[idx], header[idx]), \n",
    "                      range(0,len(old_columns)), raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 rows of DataFrame\n",
    "formatted_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a table so we can run SparkSQL queries on it\n",
    "formatted_df.registerTempTable(\"credit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query\n",
    "result = sqlContext.sql(\"select * from credit where Column1>60.0\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to create some basic plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_filtered_data = lambda res, col: res.select(col).rdd.flatMap(list).collect()\n",
    "filt_col3 = get_filtered_data(result, \"Column3\")\n",
    "filt_col4 = get_filtered_data(result, \"Column4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(filt_col3, filt_col4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
